{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tensorflow Data API\n",
    "\n",
    "- Data API: criamos um objeto do tipo dataset e dizemos para ele onde obter os dados e como transformá-los.\n",
    "- O TF toma conta de toda a implementação.\n",
    "- O Data API trabalha perfeitamente bem com o tf.keras.\n",
    "- Vamos trabalhar com Data API, TFRecorder e como criar camadas de pré-processamento customizadas.\n",
    "- TF Transforms (tf.transform): Torna possível escrever uma única função de pré-processamento que pode ser executada em batch sobre todo o conjunto de treino.\n",
    "- TF Dataset (TFDS): Fonte de datasets de onde podemos baixar alguns datasets mais comuns."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import keras.backend\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import tensorflow as tf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "X = tf.range(10) # dados de treino qualquer\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X) # criamos um dataset a partir do X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Tensor: shape=(), dtype=int32, numpy=0>\n",
      "<tf.Tensor: shape=(), dtype=int32, numpy=1>\n",
      "<tf.Tensor: shape=(), dtype=int32, numpy=2>\n",
      "<tf.Tensor: shape=(), dtype=int32, numpy=3>\n",
      "<tf.Tensor: shape=(), dtype=int32, numpy=4>\n",
      "<tf.Tensor: shape=(), dtype=int32, numpy=5>\n",
      "<tf.Tensor: shape=(), dtype=int32, numpy=6>\n",
      "<tf.Tensor: shape=(), dtype=int32, numpy=7>\n",
      "<tf.Tensor: shape=(), dtype=int32, numpy=8>\n",
      "<tf.Tensor: shape=(), dtype=int32, numpy=9>\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    pprint(item)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transformações encadeadas\n",
    "\n",
    "Visto que tenhamos um dataset, podemos aplicar qualquer tipo de transformação sobre ele.\n",
    "\n",
    "Obs.: cada método retorna um novo dataset.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# no código abaixo, repetimos o dataset 3 vezes (os dados são copiados para memória 3x) e então criarmos um batch de tamanho 7\n",
    "dataset = dataset.repeat(3).batch(7)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![chaining_transofrms](https://miro.medium.com/max/720/1*pQPYe49MSBau47N8fRaZ1w.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int32)\n",
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int32)\n",
      "tf.Tensor([16 18], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Podemos ainda realizar transformações sobre o dataset usando o método map()\n",
    "dataset = dataset.map(lambda x: x * 2)\n",
    "for item in dataset:\n",
    "    print(item)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jrste\\AppData\\Local\\Temp\\ipykernel_30692\\1259128569.py:2: unbatch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.unbatch()`.\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n",
      "tf.Tensor(12, shape=(), dtype=int32)\n",
      "tf.Tensor(14, shape=(), dtype=int32)\n",
      "tf.Tensor(16, shape=(), dtype=int32)\n",
      "tf.Tensor(18, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n",
      "tf.Tensor(12, shape=(), dtype=int32)\n",
      "tf.Tensor(14, shape=(), dtype=int32)\n",
      "tf.Tensor(16, shape=(), dtype=int32)\n",
      "tf.Tensor(18, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n",
      "tf.Tensor(12, shape=(), dtype=int32)\n",
      "tf.Tensor(14, shape=(), dtype=int32)\n",
      "tf.Tensor(16, shape=(), dtype=int32)\n",
      "tf.Tensor(18, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Se quisermos aplicar uma transformação sobre o dataset todo, usamos o apply()\n",
    "dataset = dataset.apply(tf.data.experimental.unbatch())\n",
    "for item in dataset:\n",
    "    print(item)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Podemos ainda aplicar um filtro (mask) sobre o dataset usando o filter()\n",
    "dataset = dataset.filter(lambda x: x< 10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sabemos que o algoritmos Gradient Descent trabalha melhor quando as instâncias de treino são independentes e identicamente distribuídas.\n",
    "\n",
    "Uma boa maneira de garantirmos esta condição é realizar um shuffle das instâncias.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "dataset_new = tf.data.Dataset.range(10).repeat(3)\n",
    "dataset_new = dataset_new.shuffle(buffer_size=5, seed=42).batch(7)  # o valor para o buffer_size tem de ser escolhido com cuidado para não estourar a RAM"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 2 3 6 7 9 4], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 0 1 1 8 6 5], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 8 7 1 2 3 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 4 2 7 8 9 9], shape=(7,), dtype=int64)\n",
      "tf.Tensor([3 6], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset_new:\n",
    "    print(item)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pré-Processamento dos Dados\n",
    "\n",
    "Vamos criar uma função para realizar o pré-processamento."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_mean [ 3.89175860e+00  2.86245478e+01  5.45593655e+00  1.09963474e+00\n",
      "  1.42428122e+03  2.95886657e+00  3.56464315e+01 -1.19584363e+02]\n",
      "X_std: [1.90927329e+00 1.26409177e+01 2.55038070e+00 4.65460128e-01\n",
      " 1.09576000e+03 2.36138048e+00 2.13456672e+00 2.00093304e+00]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_mean = scaler.mean_\n",
    "X_std = scaler.scale_\n",
    "\n",
    "print(f\"X_mean {X_mean}\\nX_std: {X_std}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[4.526],\n",
      "       [3.585],\n",
      "       [3.521],\n",
      "       ...,\n",
      "       [0.923],\n",
      "       [0.847],\n",
      "       [0.894]])\n"
     ]
    }
   ],
   "source": [
    "# Precisamos realizar um reshape sobre o housing.target para ser q coluna e X linhas.\n",
    "t = housing.target.reshape(-1, 1)\n",
    "pprint(t)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "(11610, 1)"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "# Quando o dataset é muito grande para caber na memória, o usual é divide-lo em uma\n",
    "# série de arquivos menores.\n",
    "def split_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\n",
    "    housing_dir = \"../data_raw\"\n",
    "    path_format = os.path.join(housing_dir, \"{}_{:02d}.csv\")\n",
    "\n",
    "    file_paths = []\n",
    "    m = len(data)\n",
    "    for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n",
    "        part_csv = path_format.format(name_prefix, file_idx)\n",
    "        file_paths.append(part_csv)\n",
    "        with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n",
    "            if header is not None:\n",
    "                f.write(header)\n",
    "                f.write(\"\\n\")\n",
    "            for row_idx in row_indices:\n",
    "                f.write(\",\".join([repr(col) for col in data[row_idx]]))\n",
    "                f.write(\"\\n\")\n",
    "    return file_paths"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "train_data = np.c_[X_train, y_train]\n",
    "valid_data = np.c_[X_valid, y_valid]\n",
    "test_data = np.c_[X_test, y_test]\n",
    "header_cols = housing.feature_names + [\"MedianHousingValues\"]\n",
    "header = \",\".join(header_cols)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "train_filepaths = split_multiple_csv_files(train_data, \"train\", header=header, n_parts=20)\n",
    "valid_filepaths = split_multiple_csv_files(valid_data, \"valid\", header=header, n_parts=20)\n",
    "test_filepaths = split_multiple_csv_files(test_data, \"test\", header=header, n_parts=20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n0  1.6571      34.0  4.454976   1.087678      1358.0  3.218009     37.94   \n1  3.6301       7.0  5.766690   1.070274      4399.0  3.091356     33.25   \n2  6.2516       9.0  6.972376   0.972376      1139.0  3.146409     34.15   \n3  2.0077      20.0  4.305195   1.099567      1452.0  3.142857     32.75   \n4  2.3567      26.0  2.292829   1.119522      2660.0  2.649402     34.07   \n\n   Longitude  MedianHousingValues  \n0    -122.35                1.052  \n1    -117.32                1.709  \n2    -117.22                1.593  \n3    -117.08                1.183  \n4    -118.29                2.531  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MedInc</th>\n      <th>HouseAge</th>\n      <th>AveRooms</th>\n      <th>AveBedrms</th>\n      <th>Population</th>\n      <th>AveOccup</th>\n      <th>Latitude</th>\n      <th>Longitude</th>\n      <th>MedianHousingValues</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.6571</td>\n      <td>34.0</td>\n      <td>4.454976</td>\n      <td>1.087678</td>\n      <td>1358.0</td>\n      <td>3.218009</td>\n      <td>37.94</td>\n      <td>-122.35</td>\n      <td>1.052</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3.6301</td>\n      <td>7.0</td>\n      <td>5.766690</td>\n      <td>1.070274</td>\n      <td>4399.0</td>\n      <td>3.091356</td>\n      <td>33.25</td>\n      <td>-117.32</td>\n      <td>1.709</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6.2516</td>\n      <td>9.0</td>\n      <td>6.972376</td>\n      <td>0.972376</td>\n      <td>1139.0</td>\n      <td>3.146409</td>\n      <td>34.15</td>\n      <td>-117.22</td>\n      <td>1.593</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2.0077</td>\n      <td>20.0</td>\n      <td>4.305195</td>\n      <td>1.099567</td>\n      <td>1452.0</td>\n      <td>3.142857</td>\n      <td>32.75</td>\n      <td>-117.08</td>\n      <td>1.183</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.3567</td>\n      <td>26.0</td>\n      <td>2.292829</td>\n      <td>1.119522</td>\n      <td>2660.0</td>\n      <td>2.649402</td>\n      <td>34.07</td>\n      <td>-118.29</td>\n      <td>2.531</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uma rápida checagem\n",
    "import pandas as pd\n",
    "pd.read_csv(train_filepaths[2]).head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "# por padrão, o list_files() retorna um dataset que mistura os caminhos dos arquivos\n",
    "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Tensor: shape=(), dtype=string, numpy=b'..\\\\data_raw\\\\train_15.csv'>\n",
      "<tf.Tensor: shape=(), dtype=string, numpy=b'..\\\\data_raw\\\\train_08.csv'>\n",
      "<tf.Tensor: shape=(), dtype=string, numpy=b'..\\\\data_raw\\\\train_03.csv'>\n",
      "<tf.Tensor: shape=(), dtype=string, numpy=b'..\\\\data_raw\\\\train_01.csv'>\n",
      "<tf.Tensor: shape=(), dtype=string, numpy=b'..\\\\data_raw\\\\train_10.csv'>\n",
      "<tf.Tensor: shape=(), dtype=string, numpy=b'..\\\\data_raw\\\\train_05.csv'>\n",
      "<tf.Tensor: shape=(), dtype=string, numpy=b'..\\\\data_raw\\\\train_19.csv'>\n",
      "<tf.Tensor: shape=(), dtype=string, numpy=b'..\\\\data_raw\\\\train_16.csv'>\n",
      "<tf.Tensor: shape=(), dtype=string, numpy=b'..\\\\data_raw\\\\train_02.csv'>\n",
      "<tf.Tensor: shape=(), dtype=string, numpy=b'..\\\\data_raw\\\\train_09.csv'>\n",
      "<tf.Tensor: shape=(), dtype=string, numpy=b'..\\\\data_raw\\\\train_00.csv'>\n",
      "<tf.Tensor: shape=(), dtype=string, numpy=b'..\\\\data_raw\\\\train_07.csv'>\n",
      "<tf.Tensor: shape=(), dtype=string, numpy=b'..\\\\data_raw\\\\train_12.csv'>\n",
      "<tf.Tensor: shape=(), dtype=string, numpy=b'..\\\\data_raw\\\\train_04.csv'>\n",
      "<tf.Tensor: shape=(), dtype=string, numpy=b'..\\\\data_raw\\\\train_17.csv'>\n",
      "<tf.Tensor: shape=(), dtype=string, numpy=b'..\\\\data_raw\\\\train_11.csv'>\n",
      "<tf.Tensor: shape=(), dtype=string, numpy=b'..\\\\data_raw\\\\train_14.csv'>\n",
      "<tf.Tensor: shape=(), dtype=string, numpy=b'..\\\\data_raw\\\\train_18.csv'>\n",
      "<tf.Tensor: shape=(), dtype=string, numpy=b'..\\\\data_raw\\\\train_06.csv'>\n",
      "<tf.Tensor: shape=(), dtype=string, numpy=b'..\\\\data_raw\\\\train_13.csv'>\n"
     ]
    }
   ],
   "source": [
    "for filepath in filepath_dataset:\n",
    "    pprint(filepath)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Usamos o método `interleave()` para ler um cinco linhas de um arquivo de uma só vez e intercalar suas linhas."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "n_reads = 5\n",
    "dataset = filepath_dataset.interleave(\n",
    "    lambda _filepath: tf.data.TextLineDataset(_filepath).skip(1),\n",
    "    cycle_length=n_reads\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(b'4.6477,38.0,5.03728813559322,0.911864406779661,745.0,2.5254237288135593,32.6'\n",
      " b'4,-117.07,1.504')\n",
      "(b'8.72,44.0,6.163179916317992,1.0460251046025104,668.0,2.794979079497908,34.2,'\n",
      " b'-118.18,4.159')\n",
      "(b'3.8456,35.0,5.461346633416459,0.9576059850374065,1154.0,2.8778054862842892,3'\n",
      " b'7.96,-122.05,1.598')\n",
      "(b'3.3456,37.0,4.514084507042254,0.9084507042253521,458.0,3.2253521126760565,36'\n",
      " b'.67,-121.7,2.526')\n",
      "(b'3.6875,44.0,4.524475524475524,0.993006993006993,457.0,3.195804195804196,34.0'\n",
      " b'4,-118.15,1.625')\n"
     ]
    }
   ],
   "source": [
    "for line in dataset.take(5):\n",
    "    pprint(line.numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "n_inputs = X_train.shape[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def preprocessing(line):\n",
    "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    x = tf.stack(fields[:-1]) # de 0 a 7 -- excluímos o 8\n",
    "    y = tf.stack(fields[-1:]) # apenas o 8\n",
    "\n",
    "    return (x - X_mean) / X_std, y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n array([ 0.20858265,  1.216324  , -0.04153753, -0.4074135 , -0.5277444 ,\n        -0.26631314,  0.8543046 , -1.3072058 ], dtype=float32)>,\n <tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.786], dtype=float32)>)"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing(b'4.290, 44.0, 5.35, 0.91, 846.0, 2.33, 37.47, -122.2, 2.786')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vamos juntar tudo em uma única função seguindo o pipeline abaixo.\n",
    "\n",
    "![pipeline](https://miro.medium.com/max/720/1*lO-22xUVBSwHvt3Zrd-CJw.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "def read_dataset(filepaths, repeat=1,\n",
    "                 n_readrs=5,\n",
    "                 n_read_threads=None,\n",
    "                 shuffle_buff_size=1000,\n",
    "                 n_parse_threads=5,\n",
    "                 batch_size=32):\n",
    "    dataset = tf.data.Dataset.list_files(filepaths).repeat(repeat)\n",
    "    dataset = dataset.interleave(lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "                                 cycle_length=n_readrs,\n",
    "                                 num_parallel_calls=n_read_threads)\n",
    "    dataset = dataset.shuffle(shuffle_buff_size)\n",
    "    dataset = dataset.map(preprocessing, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.batch(batch_size=batch_size)\n",
    "\n",
    "    return dataset.prefetch(1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sobre o `prefetch()`: quando o chamamos ao final do, criamos um dataset que irá \"se esforçar\" para sempre estar um batch a frente.\n",
    "\n",
    "![pipeline](https://miro.medium.com/max/720/1*r7jxg8IJ88MWrC-IA-q1eg.png)\n",
    "\n",
    "\n",
    "Traduzindo: enquanto nosso algoritmo de treino está trabalhando em um batch, o dataset irá trabalhar em paralelo para ter o próximo batch já pronto."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X = [[ 0.5804519  -0.20762321  0.05616303 -0.15191229  0.01343246  '\n",
      " '0.00604472\\n'\n",
      " '   1.2525111  -1.3671792 ]\\n'\n",
      " ' [-0.21650054 -0.91959685 -0.37069115 -0.15282531  1.9280853   0.6030511\\n'\n",
      " '  -0.7338394   0.8018072 ]\\n'\n",
      " ' [-0.37792316 -0.2867314  -0.44674355 -0.02454283  1.0081758  -0.3701026\\n'\n",
      " '   0.79340184 -1.1822641 ]]')\n",
      "'y = [[1.752]\\n [1.522]\\n [2.561]]\\n'\n",
      "('X = [[-1.0393791   0.02970133  0.0704432   0.01656396 -0.14901187  '\n",
      " '0.2554778\\n'\n",
      " '   0.69033587 -0.2876847 ]\\n'\n",
      " ' [-0.47738516  0.10880951 -0.23843908 -0.0527132   0.29999155  0.21409526\\n'\n",
      " '  -0.67293835  0.5919061 ]\\n'\n",
      " ' [ 0.6041258   0.8998913  -0.00953289  0.17155597 -0.63817006  0.05904346\\n'\n",
      " '  -0.8181658   0.6118972 ]]')\n",
      "'y = [[0.792]\\n [1.915]\\n [2.319]]\\n'\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "train_set = read_dataset(train_filepaths, batch_size=3)\n",
    "for X_batch, y_batch in train_set.take(2):\n",
    "    pprint(f\"X = {X_batch}\")\n",
    "    pprint(f\"y = {y_batch}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vamos agora para a última parte, em que juntamos com o `tf.keras`.\n",
    "\n",
    "Usamos a função `read_dataset()` que acabamos de criar para gerar um conjunto de treino, um de validação e um de teste."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "train_set = read_dataset(train_filepaths, repeat=None)\n",
    "test_set = read_dataset(test_filepaths)\n",
    "valid_set = read_dataset(valid_filepaths)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Agora podemos construir e treinar um modelo com o Keras usando estes datasets passando os de treino e validação para o `fit()`"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(32, 8), dtype=float32, numpy=\n",
      "array([[ 9.40889344e-02, -1.55246222e+00,  4.79670703e-01,\n",
      "        -3.21188346e-02,  1.61275044e-01, -3.14145871e-02,\n",
      "         1.40711010e+00, -6.92494690e-01],\n",
      "       [-1.04775929e+00,  1.13721585e+00,  1.67430863e-02,\n",
      "         2.59568132e-02, -2.85903156e-01,  2.89581902e-02,\n",
      "         9.90162194e-01, -1.29721212e+00],\n",
      "       [-2.78356493e-01, -1.39424586e+00,  2.80269951e-01,\n",
      "        -1.49353482e-02, -7.76886582e-01, -6.93427771e-02,\n",
      "         1.69756675e+00, -8.22435141e-01],\n",
      "       [ 2.32650900e+00, -3.65839571e-01,  9.64194715e-01,\n",
      "        -8.51519182e-02, -3.13281417e-01, -1.08566359e-01,\n",
      "         9.43315029e-01, -1.17227042e+00],\n",
      "       [-1.04168367e+00,  5.83458602e-01, -6.80176437e-01,\n",
      "        -1.12405427e-01,  1.48820794e+00,  4.01368171e-01,\n",
      "        -7.47894943e-01,  9.11756575e-01],\n",
      "       [-9.01996970e-01,  5.83458602e-01, -5.05193770e-01,\n",
      "         1.08205207e-01,  1.32941401e+00,  2.72037178e-01,\n",
      "        -7.29155362e-01,  7.11849034e-01],\n",
      "       [-1.23599827e+00, -6.82272315e-01, -3.66222471e-01,\n",
      "        -1.66942134e-01, -1.27109259e-01, -5.96617758e-02,\n",
      "         6.85651839e-01, -3.32661897e-01],\n",
      "       [-4.20295388e-01,  1.05810761e+00, -3.01651716e-01,\n",
      "        -9.16399732e-02, -2.40272731e-01,  1.47721082e-01,\n",
      "        -8.27535570e-01,  7.06854105e-01],\n",
      "       [ 1.38012803e+00,  1.08809508e-01,  2.26754382e-01,\n",
      "        -4.27426130e-01, -6.05316162e-01, -1.49362683e-01,\n",
      "        -6.63568556e-01,  5.51923811e-01],\n",
      "       [-7.72576034e-01,  1.87917694e-01, -5.29596686e-01,\n",
      "        -1.70566350e-01, -3.28795761e-01,  5.71202278e-01,\n",
      "        -7.57264733e-01,  7.06854105e-01],\n",
      "       [ 9.88669991e-01,  5.04350424e-01,  1.47498306e-02,\n",
      "        -2.81318069e-01,  1.73138961e-01,  5.72082074e-03,\n",
      "        -7.10415781e-01,  6.81864262e-01],\n",
      "       [ 3.36013317e-01,  4.25242215e-01,  6.49953187e-01,\n",
      "         3.62015963e-01, -5.17705739e-01, -1.08155631e-01,\n",
      "        -7.24471331e-01,  4.16988403e-01],\n",
      "       [-1.28261304e+00,  2.67025858e-01, -5.02348125e-01,\n",
      "        -2.48820454e-01, -5.22268772e-01, -8.53674561e-02,\n",
      "         2.68705696e-01,  1.17129050e-01],\n",
      "       [ 6.77713990e-01, -8.40488672e-01,  3.20790410e-01,\n",
      "        -1.66313902e-01, -1.35322750e-01,  8.12028199e-02,\n",
      "         1.47738099e+00, -7.97445238e-01],\n",
      "       [-7.58277357e-01, -9.19596851e-01, -6.01783812e-01,\n",
      "        -3.08451969e-02,  3.29243517e+00,  4.61341828e-01,\n",
      "        -9.07177925e-01,  1.67639923e+00],\n",
      "       [ 2.60906368e-01,  3.46134037e-01, -2.52425700e-01,\n",
      "        -1.38881892e-01,  1.57490575e+00,  1.20363414e+00,\n",
      "        -9.02492166e-01,  8.41789484e-01],\n",
      "       [ 1.23530841e+00, -2.10621953e+00,  1.15756309e+00,\n",
      "         1.90780148e-01,  2.75034571e+00,  2.54855752e-02,\n",
      "         1.41179419e+00, -7.72455394e-01],\n",
      "       [-1.04121220e+00,  5.04350424e-01, -5.10063529e-01,\n",
      "        -1.73845604e-01,  5.72376698e-02,  4.30709630e-01,\n",
      "        -7.33839393e-01,  6.96856618e-01],\n",
      "       [-9.89517152e-01,  2.67025858e-01, -3.60718906e-01,\n",
      "        -8.98182541e-02,  1.10856283e+00,  3.23233485e-01,\n",
      "         1.10407416e-02,  1.72103703e-01],\n",
      "       [ 1.11421525e+00, -1.63157046e+00,  3.49512666e-01,\n",
      "        -1.11953646e-01,  1.28520098e+01,  4.73563746e-02,\n",
      "        -7.57264733e-01,  9.01762903e-01],\n",
      "       [-1.44064635e-01,  2.97013279e-02, -8.89980555e-01,\n",
      "        -2.04756096e-01, -7.66847908e-01, -1.82405144e-01,\n",
      "        -8.08797717e-01,  6.16895914e-01],\n",
      "       [ 2.73266882e-01,  1.08809508e-01, -2.67420292e-01,\n",
      "        -9.26244631e-02, -2.67650992e-01, -2.11812928e-01,\n",
      "        -9.35285509e-01,  8.41789484e-01],\n",
      "       [-8.20604682e-01,  5.04350424e-01, -1.11555010e-01,\n",
      "        -8.18465576e-02, -5.13627529e-02,  2.02114265e-02,\n",
      "        -1.06079042e-01,  2.67056823e-01],\n",
      "       [-1.02897048e-01, -1.23602951e+00, -4.18578267e-01,\n",
      "        -1.62344187e-01,  5.83812833e-01, -2.96716660e-01,\n",
      "         1.25719690e+00, -1.56208801e+00],\n",
      "       [-3.70800078e-01,  2.67025858e-01,  9.16407779e-02,\n",
      "         6.74595460e-02, -2.74039239e-01,  3.88330251e-01,\n",
      "        -7.29155362e-01,  1.05169070e+00],\n",
      "       [-3.65876675e-01,  1.77008128e+00, -4.11480814e-02,\n",
      "         2.98147619e-01, -7.44945288e-01, -4.00452822e-01,\n",
      "         6.01325452e-01, -1.20725393e+00],\n",
      "       [ 1.04094124e+00, -9.19596851e-01,  2.13232189e-02,\n",
      "        -1.13286696e-01,  2.16944173e-01,  6.74742013e-02,\n",
      "         8.21511209e-01, -1.19226158e+00],\n",
      "       [ 3.45283955e-01, -1.31513774e+00,  3.90285879e-01,\n",
      "        -1.37683287e-01,  4.17718053e-01, -2.11980641e-02,\n",
      "         1.32746780e+00, -1.57708037e+00],\n",
      "       [ 5.86632192e-01,  4.25242215e-01,  3.14827085e-01,\n",
      "        -1.98819458e-01, -5.95277488e-01, -9.37070251e-02,\n",
      "        -1.32412410e+00,  1.33156276e+00],\n",
      "       [ 1.68396091e+00, -9.98705029e-01,  8.73477340e-01,\n",
      "        -1.17703587e-01,  1.55847883e+00,  3.54664147e-01,\n",
      "        -9.07177925e-01,  8.96764159e-01],\n",
      "       [ 5.81809902e+00,  8.99891317e-01,  1.62488055e+00,\n",
      "        -8.63131285e-02, -3.65300119e-01, -8.10114890e-02,\n",
      "        -7.29155362e-01,  5.81912398e-01],\n",
      "       [-8.10757995e-01, -8.40488672e-01, -1.90380141e-01,\n",
      "        -1.36317968e-01, -3.30620974e-01,  2.26373717e-01,\n",
      "         1.38837051e+00, -9.47376847e-01]], dtype=float32)>,\n",
      " <tf.Tensor: shape=(32, 1), dtype=float32, numpy=\n",
      "array([[3.006  ],\n",
      "       [0.912  ],\n",
      "       [1.625  ],\n",
      "       [5.00001],\n",
      "       [0.98   ],\n",
      "       [1.708  ],\n",
      "       [0.505  ],\n",
      "       [1.627  ],\n",
      "       [2.731  ],\n",
      "       [1.255  ],\n",
      "       [2.724  ],\n",
      "       [3.773  ],\n",
      "       [0.546  ],\n",
      "       [2.201  ],\n",
      "       [0.9    ],\n",
      "       [1.807  ],\n",
      "       [2.225  ],\n",
      "       [1.395  ],\n",
      "       [0.547  ],\n",
      "       [2.539  ],\n",
      "       [2.     ],\n",
      "       [2.723  ],\n",
      "       [0.632  ],\n",
      "       [1.576  ],\n",
      "       [0.986  ],\n",
      "       [4.     ],\n",
      "       [2.555  ],\n",
      "       [1.783  ],\n",
      "       [1.653  ],\n",
      "       [3.134  ],\n",
      "       [5.00001],\n",
      "       [0.894  ]], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "# antes, vamos ver com que cara do dataset ficou\n",
    "for item in train_set.take(1):\n",
    "    pprint(item)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation='relu', input_shape=X_train.shape[1:]),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 32)                288       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 321\n",
      "Trainable params: 321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "# compile o modelo\n",
    "model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(learning_rate=1e-3))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "362/362 [==============================] - 4s 4ms/step - loss: 1.7374 - val_loss: 1.2373\n",
      "Epoch 2/20\n",
      "362/362 [==============================] - 1s 2ms/step - loss: 0.8314 - val_loss: 0.7371\n",
      "Epoch 3/20\n",
      "362/362 [==============================] - 1s 2ms/step - loss: 0.7255 - val_loss: 0.6808\n",
      "Epoch 4/20\n",
      "362/362 [==============================] - 1s 2ms/step - loss: 0.6840 - val_loss: 0.6677\n",
      "Epoch 5/20\n",
      "362/362 [==============================] - 1s 2ms/step - loss: 0.6443 - val_loss: 0.7693\n",
      "Epoch 6/20\n",
      "362/362 [==============================] - 1s 2ms/step - loss: 0.6075 - val_loss: 0.6610\n",
      "Epoch 7/20\n",
      "362/362 [==============================] - 1s 2ms/step - loss: 0.5831 - val_loss: 0.5864\n",
      "Epoch 8/20\n",
      "362/362 [==============================] - 1s 2ms/step - loss: 0.5555 - val_loss: 0.5805\n",
      "Epoch 9/20\n",
      "362/362 [==============================] - 1s 2ms/step - loss: 0.5377 - val_loss: 0.5259\n",
      "Epoch 10/20\n",
      "362/362 [==============================] - 1s 2ms/step - loss: 0.5213 - val_loss: 0.5068\n",
      "Epoch 11/20\n",
      "362/362 [==============================] - 1s 2ms/step - loss: 0.5044 - val_loss: 0.4844\n",
      "Epoch 12/20\n",
      "362/362 [==============================] - 1s 2ms/step - loss: 0.4878 - val_loss: 0.4690\n",
      "Epoch 13/20\n",
      "362/362 [==============================] - 1s 2ms/step - loss: 0.4815 - val_loss: 0.4580\n",
      "Epoch 14/20\n",
      "362/362 [==============================] - 1s 2ms/step - loss: 0.4718 - val_loss: 0.4527\n",
      "Epoch 15/20\n",
      "362/362 [==============================] - 1s 2ms/step - loss: 0.4614 - val_loss: 0.4460\n",
      "Epoch 16/20\n",
      "362/362 [==============================] - 1s 2ms/step - loss: 0.4570 - val_loss: 0.4326\n",
      "Epoch 17/20\n",
      "362/362 [==============================] - 1s 2ms/step - loss: 0.4503 - val_loss: 0.4234\n",
      "Epoch 18/20\n",
      "362/362 [==============================] - 1s 2ms/step - loss: 0.4504 - val_loss: 0.4227\n",
      "Epoch 19/20\n",
      "362/362 [==============================] - 1s 2ms/step - loss: 0.4433 - val_loss: 0.4132\n",
      "Epoch 20/20\n",
      "362/362 [==============================] - 1s 2ms/step - loss: 0.4369 - val_loss: 0.4120\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x1f37f042520>"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model.fit(train_set, steps_per_epoch=len(X_train) // batch_size,\n",
    "          epochs=20,\n",
    "          validation_data=valid_set)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4319\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.43188807368278503"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# avaliação do modelo\n",
    "model.evaluate(test_set, steps=len(X_test) // batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "# Agora podemos fazer predições\n",
    "new_set = test_set.map(lambda X, y: X) # Queremos apenas os valores e não as labels. O keras teria ignorado mesmo assim."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "X_new = model.predict(new_set, steps=len(X_test) // batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1.7827599],\n       [2.4168222],\n       [2.1397085],\n       ...,\n       [3.1641123],\n       [2.0432794],\n       [1.8787067]], dtype=float32)"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}